{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\matplotlib\\projections\\__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "import numpy as np\n",
    "from modAL.models import ActiveLearner\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "cifar100 = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(cifar100, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training data shape: (600, 3072), Labels shape: (600,)\n",
      "Test data shape: (150, 3072), Labels shape: (150,)\n"
     ]
    }
   ],
   "source": [
    "def load_filtered_CIFAR(selected_labels, num_train_per_class=200, num_test_per_class=50):\n",
    "    '''\n",
    "    Loads CIFAR-100 dataset but filters it to only include specified labels with a limited number of samples.\n",
    "\n",
    "    :param selected_labels: List of 3 labels to keep\n",
    "    :param num_train_per_class: Number of samples per label for training (default: 200)\n",
    "    :param num_test_per_class: Number of samples per label for testing (default: 50)\n",
    "    :return: Filtered training and test sets -> (X_train, y_train, X_test, y_test)\n",
    "    '''\n",
    "\n",
    "    # Load CIFAR-100 dataset\n",
    "    train_set = datasets.CIFAR100(root='./data', train=True, download=True)\n",
    "    test_set = datasets.CIFAR100(root='./data', train=False, download=True)\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    X_train, y_train = train_set.data, np.array(train_set.targets)\n",
    "    X_test, y_test = test_set.data, np.array(test_set.targets)\n",
    "\n",
    "    # Function to filter data\n",
    "    def filter_data(X, y, num_samples_per_class):\n",
    "        filtered_images = []\n",
    "        filtered_labels = []\n",
    "        \n",
    "        for label in selected_labels:\n",
    "            indices = np.where(y == label)[0]  # Get indices for the label\n",
    "            selected_indices = indices[:num_samples_per_class]  # Take only required samples\n",
    "            \n",
    "            filtered_images.append(X[selected_indices])\n",
    "            filtered_labels.append(y[selected_indices])\n",
    "\n",
    "        # Stack and flatten images\n",
    "        X_filtered = np.concatenate(filtered_images, axis=0).reshape(-1, 32 * 32 * 3).astype(np.float32)\n",
    "        y_filtered = np.concatenate(filtered_labels, axis=0)\n",
    "\n",
    "        return X_filtered, y_filtered\n",
    "\n",
    "    # Filter training and test sets\n",
    "    X_train_filtered, y_train_filtered = filter_data(X_train, y_train, num_train_per_class)\n",
    "    X_test_filtered, y_test_filtered = filter_data(X_test, y_test, num_test_per_class)\n",
    "\n",
    "    return X_train_filtered, y_train_filtered, X_test_filtered, y_test_filtered\n",
    "\n",
    "# Select 3 labels (e.g., labels 0, 1, and 2)\n",
    "selected_labels = [0, 1, 2]\n",
    "X_train, y_train, X_test, y_test = load_filtered_CIFAR(selected_labels, num_train_per_class=200, num_test_per_class=50)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"Training data shape: {X_train.shape}, Labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}, Labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3,32,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32*15*15,10)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_members = [2, 4, 8, 16]\n",
    "n_repeats = 3\n",
    "n_queries = 100\n",
    "\n",
    "# permutations=[np.random.permutation(X_train.shape[0]) for _ in range(n_repeats)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1           nan       \u001b[32m0.0850\u001b[0m           nan  0.5261\n",
      "      2           nan       0.0850           nan  0.0729\n",
      "      3           nan       0.0850           nan  0.0691\n",
      "      4           nan       0.0850           nan  0.0610\n",
      "      5           nan       0.0850           nan  0.0619\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "the dimensions of the new training data and label mustagree with the training data and labels provided so far",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\modAL\\models\\learners.py:109\u001b[0m, in \u001b[0;36mActiveLearner._add_training_data\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_training \u001b[38;5;241m=\u001b[39m data_vstack((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_training, X))\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_training \u001b[38;5;241m=\u001b[39m \u001b[43mdata_vstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\modAL\\utils\\data.py:31\u001b[0m, in \u001b[0;36mdata_vstack\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(blocks[\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(blocks[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m learner \u001b[38;5;241m=\u001b[39m ActiveLearner(estimator\u001b[38;5;241m=\u001b[39mnet, X_training\u001b[38;5;241m=\u001b[39mX_init, y_training\u001b[38;5;241m=\u001b[39my_init)\n\u001b[0;32m     25\u001b[0m query_idx, _ \u001b[38;5;241m=\u001b[39m learner\u001b[38;5;241m.\u001b[39mquery(X_pool)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mteach\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_pool\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mX_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43my_pool\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\modAL\\models\\learners.py:175\u001b[0m, in \u001b[0;36mActiveLearner.teach\u001b[1;34m(self, X, y, bootstrap, only_new, **fit_kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03mAdds X and y to the known training data and retrains the predictor with the augmented dataset.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03m    **fit_kwargs: Keyword arguments to be passed to the fit method of the predictor.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m only_new:\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_training_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_to_known(bootstrap\u001b[38;5;241m=\u001b[39mbootstrap, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\modAL\\models\\learners.py:111\u001b[0m, in \u001b[0;36mActiveLearner._add_training_data\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_training \u001b[38;5;241m=\u001b[39m data_vstack((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_training, y))\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe dimensions of the new training data and label must\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    112\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124magree with the training data and labels provided so far\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: the dimensions of the new training data and label mustagree with the training data and labels provided so far"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "cifar = datasets.CIFAR10(root='./data',train=True,download=True,transform=transform)\n",
    "indices = np.arange(len(cifar))\n",
    "np.random.shuffle(indices)\n",
    "init_idx = indices[:1000]\n",
    "pool_idx = indices[1000:3000]\n",
    "\n",
    "def load_subset(dataset,indices):\n",
    "    subset = Subset(dataset,indices)\n",
    "    loader = DataLoader(subset,batch_size=len(subset))\n",
    "    data,labels = next(iter(loader))\n",
    "    return data.numpy(), labels.numpy()\n",
    "\n",
    "X_init,y_init = load_subset(cifar,init_idx)\n",
    "X_pool,y_pool = load_subset(cifar,pool_idx)\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    SimpleCNN,\n",
    "    max_epochs=5,\n",
    "    lr=0.001,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "learner = ActiveLearner(estimator=net, X_training=X_init, y_training=y_init)\n",
    "query_idx, _ = learner.query(X_pool)\n",
    "learner.teach(X_pool[query_idx].reshape(1,*X_pool.shape[1:]), [y_pool[query_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in tqdm(range(2)):\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
