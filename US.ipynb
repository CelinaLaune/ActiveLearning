{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "import numpy as np\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define the transformation to convert images to PyTorch tensors\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load the MNIST dataset with the specified transformation\n",
    "cifar100 = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Create a DataLoader to load the dataset in batches\n",
    "train_loader = torch.utils.data.DataLoader(cifar100, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training data shape: (50000, 3072), Labels shape: (50000,)\n",
      "Test data shape: (10000, 3072), Labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "def load_CIFAR():\n",
    "    '''\n",
    "    Function to load the cifar100 training and test set with corresponding labels.\n",
    "\n",
    "    :return: training_examples, training_labels, test_examples, test_labels\n",
    "    '''\n",
    "\n",
    "    # we want to flat the examples\n",
    "\n",
    "    training_set = datasets.CIFAR100(root='./data', train=True, download=True, transform= None)\n",
    "    test_set = datasets.CIFAR100(root='./data', train=False, download=True, transform= None)\n",
    "\n",
    "    Xtrain = training_set.data.reshape(-1,32*32*3)\n",
    "    Xtest = test_set.data.reshape(-1,32*32*3)\n",
    "\n",
    "    ytrain = np.array(training_set.targets)\n",
    "    ytest = np.array(test_set.targets)\n",
    "\n",
    "    return Xtrain, ytrain, Xtest, ytest\n",
    "\n",
    "Xtrain, ytrain, Xtest, ytest = load_CIFAR()\n",
    "\n",
    "# Print shapes for verification\n",
    "print(f\"Training data shape: {Xtrain.shape}, Labels shape: {ytrain.shape}\")\n",
    "print(f\"Test data shape: {Xtest.shape}, Labels shape: {ytest.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training data shape: (600, 3072), Labels shape: (600,)\n",
      "Test data shape: (150, 3072), Labels shape: (150,)\n"
     ]
    }
   ],
   "source": [
    "def load_filtered_CIFAR(selected_labels, num_train_per_class=200, num_test_per_class=50):\n",
    "    '''\n",
    "    Loads CIFAR-100 dataset but filters it to only include specified labels with a limited number of samples.\n",
    "\n",
    "    :param selected_labels: List of 3 labels to keep\n",
    "    :param num_train_per_class: Number of samples per label for training (default: 200)\n",
    "    :param num_test_per_class: Number of samples per label for testing (default: 50)\n",
    "    :return: Filtered training and test sets -> (X_train, y_train, X_test, y_test)\n",
    "    '''\n",
    "\n",
    "    # Load CIFAR-100 dataset\n",
    "    train_set = datasets.CIFAR100(root='./data', train=True, download=True)\n",
    "    test_set = datasets.CIFAR100(root='./data', train=False, download=True)\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    X_train, y_train = train_set.data, np.array(train_set.targets)\n",
    "    X_test, y_test = test_set.data, np.array(test_set.targets)\n",
    "\n",
    "    # Function to filter data\n",
    "    def filter_data(X, y, num_samples_per_class):\n",
    "        filtered_images = []\n",
    "        filtered_labels = []\n",
    "        \n",
    "        for label in selected_labels:\n",
    "            indices = np.where(y == label)[0]  # Get indices for the label\n",
    "            selected_indices = indices[:num_samples_per_class]  # Take only required samples\n",
    "            \n",
    "            filtered_images.append(X[selected_indices])\n",
    "            filtered_labels.append(y[selected_indices])\n",
    "\n",
    "        # Stack and flatten images\n",
    "        X_filtered = np.concatenate(filtered_images, axis=0).reshape(-1, 32 * 32 * 3).astype(np.float32)\n",
    "        y_filtered = np.concatenate(filtered_labels, axis=0)\n",
    "\n",
    "        return X_filtered, y_filtered\n",
    "\n",
    "    # Filter training and test sets\n",
    "    X_train_filtered, y_train_filtered = filter_data(X_train, y_train, num_train_per_class)\n",
    "    X_test_filtered, y_test_filtered = filter_data(X_test, y_test, num_test_per_class)\n",
    "\n",
    "    return X_train_filtered, y_train_filtered, X_test_filtered, y_test_filtered\n",
    "\n",
    "# Select 3 labels (e.g., labels 0, 1, and 2)\n",
    "selected_labels = [0, 1, 2]\n",
    "X_train, y_train, X_test, y_test = load_filtered_CIFAR(selected_labels, num_train_per_class=200, num_test_per_class=50)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"Training data shape: {X_train.shape}, Labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}, Labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee4a702f15f4aeca36740a4e5eaa971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9875\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in tqdm(range(2)):\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
